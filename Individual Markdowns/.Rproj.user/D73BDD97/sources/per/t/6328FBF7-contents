---
title: "STA108 HW3"
author: "Gabriel Jones"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=F}
library(readr)
library(ggplot2)
library(dplyr)
library(RColorBrewer)
library(knitr)
library(plotrix)
library(ggpubr)
library(MASS)
library(lindia)
```

```{r echo=F}
# Printing one matrix
write_matex <- function(x, name) {
  if(name != ""){
    name <- paste0(name, "=")
  }
  begin <- paste0("$$", name, "\\begin{pmatrix}")
  end <- "\\end{pmatrix}$$"
  X <- apply(x, 1, function(x) {
    paste(paste(x, collapse = "&"), "\\\\")
    })
  writeLines(c(begin, X, end))
}

# Matrix Text
make_matrix <- function(x, split, name, transpose){
  if(name != ""){
    name <- paste0(name, "=")
  }
  if(transpose==T){transpose<-"'"} else {transpose<-""}
  
  begin_matrix <- paste0(name, "\\begin{pmatrix}")
  end_matrix <- "\\end{pmatrix}"
  return(c(begin_matrix, apply(x, 1, function(x) {
    paste(paste(x, collapse = "&"), "\\\\")
    }), paste0(end_matrix, transpose, split)))
}

# Printing multiple matrices
write_multi_matex <- function(lmat, split=c("hatsune"), names=c("miku"), 
                              transpose=c("")) {
  # x will be a list containing each matrix we want to display
  if(length(split)==1){split<-rep("", length(lmat))}
  if(length(names)==1){names<-rep("", length(lmat))}
  if(length(transpose)==1){transpose<-rep(F, length(lmat))}

  
  newlist <- list()
  i <- 0
  for (mat in lmat){
    i <- i+1
    newlist <- append(newlist, make_matrix(mat, split[i], names[i], transpose[i]))
  }
  
  writeLines(c("$$", paste(lapply(newlist, paste, collapse=" ")), "$$"))
  
}
```

**Note: All calculations involving matrices were written by hand. The calculations shown in this document mirror my handwritten calculations.**

## Chapter 5

**1. Suppose we have matrices**

```{r results='asis', echo=F}
A <- matrix(c(1,3,
           0,6), nrow=2, ncol=2, byrow=T)

B <- matrix(c(1,-3,
           0,-5), nrow=2, ncol=2, byrow=T)

write_multi_matex(list(A,B), names=c("A", "B"))
```

**Find $(A + B)$, $(A - B)$, $(A + B)'$, the transpose of the sum, $(A - B)'$, and the transpose of the difference.**

```{r echo=F}
# A+B
sum_AB <- matrix(c(2, 0,
                   0, 1), nrow=2, ncol=2, byrow=T)
# A-B
sub_AB <- matrix(c(0,0,
                   0,11), nrow=2, byrow=T)

# (A+B)'
trans_sum_AB <- matrix(c(2,0,
                         0,1), nrow=2, ncol=2, byrow=T)

# (A-B)'
trans_sub_AB <- matrix(c(0,0,
                         0,11), nrow=2, byrow=T)
```

```{r results='asis', echo=F}
write_multi_matex(list(A,B,sum_AB), split=c("+","=",""), names = c("A+B","",""))
write_multi_matex(list(A,B,sub_AB), split=c("-","=",""), names = c("A-B","",""))
write_multi_matex(list(sum_AB,trans_sum_AB), split=c("=",""), names = c("(A+B)'",""), transpose=c(T,F))
write_multi_matex(list(sub_AB,trans_sub_AB), split=c("=",""), names = c("(A-B)'",""), transpose=c(T,F))
```

**2. Consider the matrices**

```{r results='asis', echo=F}
A <- matrix(c(1,3,
              0,2,
              1,1), nrow=3, byrow=T)

B <- matrix(c(1,-2,
              0,2,
              0,2), nrow=3, byrow=T)

write_multi_matex(list(A,B), names=c("A","B"))
```

**Find $A'$, $B'$, $A \cdot B'$ and $A' \cdot B$, $(A \cdot B')'$ and $(A' \cdot B)'$**

```{r results='asis', echo=F}
# A'
At <- matrix(c(1,0,1,
                3,2,1), nrow=2, byrow=T)

# B'
Bt <- matrix(c(1,0,0,
                -2,2,2), nrow=2, byrow=T)

# A * B'
A.Bt <- matrix(c(-5,6,6,
                      -4,4,4,
                      -1,2,2), nrow=3, byrow=T)

# A' * B
At.B <- matrix(c(1,0,
                      3,0), nrow=2, byrow=T)

# (A * B')'
A.BtT <- matrix(c(-5,-4,-1,
                      6,4,2,
                      6,4,2), nrow=3, byrow=T)

# (A' * B)'
At.BT <- matrix(c(1,3,
                  0,0), nrow=2, byrow=T)

write_matex(At, "A'")
write_matex(Bt, "B'")
write_multi_matex(list(A,Bt,A.Bt), split=c("","=",""), names=c("A \\cdot B'","",""))
write_multi_matex(list(At,B,At.B), split=c("","=",""), names=c("A' \\cdot B","",""))
write_matex(A.BtT, "(A \\cdot B')'")
write_matex(At.BT, "(A' \\cdot B)'")
```

**3. Consider the matrices**

```{r results='asis', echo=F}
A <- matrix(c(1,3,
              0,2,
              1,1), nrow=3, byrow=T)

B <- matrix(c(1,-2,
              0,2,
              0,2), nrow=3, byrow=T)

write_multi_matex(list(A,B), names=c("A","B"))
```

**Find $B' \cdot A$ and $B \cdot A'$. Compare the results to your answers in problem 2. What do you notice?**

```{r results='asis', echo=F}
# B' * A
Bt.A <- matrix(c(1,3,
                 0,0), nrow=2, byrow=T)

# B * A'
B.At <- matrix(c(-5,-4,-1,
                 6,4,2,
                 6,4,2), nrow=3, byrow=T)

write_multi_matex(list(Bt,A,Bt.A), split=c("","=",""), names=c("B' \\cdot A","",""))
write_multi_matex(list(B,At,B.At), split=c("","=",""), names=c("B \\cdot A'","",""))
```

From this we can see that $B' \cdot A = (A' \cdot B)'$ and $B \cdot A' = (A \cdot B')'$

**4. Suppose we have the matrix**

```{r results='asis', echo=F}
A <- matrix(c(1,3,1,
              0,2,2,
              1,1,0), nrow=3, byrow=T)

write_matex(A, "A")
```

**Find the products $A \cdot I$ where $I$ is the identity matrix and $A \cdot J$ where $J$ is the unit matrix. Both $I$ and $J$ are 3 Ã— 3 matrices.**

```{r results='asis', echo=F}
J<-matrix(c(1,1,1,
            1,1,1,
            1,1,1), nrow=3, byrow=T)

I<-matrix(c(1,0,0,
           0,1,0,
           0,0,1), nrow=3, byrow=T)

A.J <- A %*% J
A.I <- A %*% I

write_multi_matex(list(A,I,A.I), split=c("","=",""), names=c("A \\cdot I", "", ""))
write_multi_matex(list(A,J,A.J), split=c("","=",""), names=c("A \\cdot J", "", ""))
```

**5. Suppose $Y$ is a random vector with elements $(Y_1, Y_2)$ and $X = (X_1, X_2)$.**

```{r results='asis', echo=F}
mean_Y <- matrix(c(1,
                   2), nrow=2, byrow=T)

mean_X <- matrix(c(2,
                   3), nrow=2, byrow=T)

write_multi_matex(list(mean_Y, mean_X), names = c("E[Y]", "E[X]"))
```

**Find $E[Y + X]$.**

$$
E[Y + X] = E[Y] + E[X] =
\begin{pmatrix}
1 \\
2 \\
\end{pmatrix}
+
\begin{pmatrix}
2 \\
3 \\
\end{pmatrix}
=
\begin{pmatrix}
3 \\
5 \\
\end{pmatrix}
$$

**6. For the random vector Y in problem 5, suppose the variance-covariance matrix**

```{r results='asis', echo=F}
sum_Y <- matrix(c(2,1,
                  1,1), nrow=2, byrow=T)

A <- matrix(c(2,1), nrow=2, byrow=T)

write_multi_matex(list(sum_Y, A), names=c("\\sum_Y", "A"))
```

**If we let $W = A'Y$ we can show that the variance of $W$ is given by $A'\sum_Y A$. Find the variance $\sum_W$? What is the dimension of $\sum_W$?**

\begin{gather*}
\sum_W = E[W \cdot W'] - E[W] \cdot E[W'] \\
= E[A'Y \cdot Y'A] - E[A'Y] \cdot E[Y'A] \\
= A'E[Y \cdot Y']A - A'E[Y] \cdot AE[Y'] \\
= A'(E[Y \cdot Y'] - E[Y] \cdot E[Y'])A \\
= A'\sum_Y A 
=
\begin{pmatrix}
2 & 1
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\
1 & 1
\end{pmatrix}
\begin{pmatrix}
2 \\ 1
\end{pmatrix}
=
13
\end{gather*}

As shown above, $\sum_W = A'\sum_Y A$. The dimensions of each matrix in the calculation are 1 x 2, 2 x 2, and 2 x 1 respectively. From this, we can deduce that the dimensions of $\sum_w$ are 1 x 1

**7. Consider the matrices A and B and their inverses given below:**

```{r results='asis', echo=F}
A <- matrix(c(1,1,
              0,2), nrow=2, byrow=T)

A_inv <- matrix(c(1, -0.5,
                0, 0.5), nrow=2, byrow=T)

B <- matrix(c(2,0,
              1,2), nrow=2, byrow=T)

B_inv <- matrix(c(0.5,0,
                  -0.25,0.5), nrow=2, byrow=T)

write_multi_matex(list(A,A_inv,B,B_inv), names=c("A","A^{-1}","B","B^{-1}"))
```

**Verify that $A^{-1}$ and $B^{-1}$ are in fact the inverses. Find $(A \cdot B)^{-1}$ by direct calculation and by using the formula given in chapter 5 on page 7.**

\begin{gather*}
A^{-1} = \frac{1}{2(1)+1(0)} 
\begin{pmatrix}
2&-1\\
0&1\\
\end{pmatrix}
=
\begin{pmatrix}
2&-0.5\\
0&0.5\\
\end{pmatrix} 
\\
B^{-1} = \frac{1}{2(2)+1(0)} 
\begin{pmatrix}
2&0\\
-1&2\\
\end{pmatrix}
=
\begin{pmatrix}
0.5&0\\
-0.25&0.5\\
\end{pmatrix} 
\\
(A \cdot B)^{-1} = B^{-1} \cdot A^{-1} = 
\begin{pmatrix}
0.5&0.25\\
-0.25&0.375\\
\end{pmatrix} 
\end{gather*}

**8. Using the data set FEV.csv use a Box-Cox transformation to identify the best transformation of FEV for the regression of FEV on age. Carry out the regression analysis with the transformed data set.**

```{r}
# Read the data
fev <- read.csv("../Data/FEV.csv")
fev <- janitor::clean_names(fev)

# Make linear model
fev.lm <- lm(fev ~ age, data=fev)

# Print Boxcox likelyhood graph 
fev.boxcox <- gg_boxcox(fev.lm, lambdaSF = 2)

# Extract lambda
bc <- boxcox(fev.lm, plotit = F)
lambda <- bc$x[which.max(bc$y)]

# Fit new model
fev.bc <- mutate(fev, fev=((fev^lambda-1)/lambda))
fev.bc.lm <- lm(fev ~ age, data=fev.bc)

# Create Scatterplot
fev.scatter <- fev.bc%>%
  ggplot(aes(x=age,y=fev))+
  geom_point(color='burlywood3')+
  geom_smooth(method='lm', color="black")+
  labs(title="Transformed FEV ~ Age Scatterplot",
       x="Age",
       y="FEV",
       subtitle = paste0("lambda=", round(lambda,3)))

# Create Residual Plot
fev.resid <- fev.bc%>%
  ggplot(aes(x=fitted(fev.bc.lm), y=residuals(fev.bc.lm)))+
  geom_point(color="darkgoldenrod", shape=21)+
  geom_hline(yintercept=0)+
  labs(title="Residuals Against Fitted Values",
       x="Fitted Values",
       y="Residuals")

# Create Residual QQ plot
fev.residQQ <- fev.bc%>%
  ggplot(aes(sample=residuals(fev.bc.lm)))+
  geom_qq(color="brown3", size=1.3)+
  geom_qq_line()+
  labs(title="Residual QQ Plot",
       x="Theoretical Quantiles",
       y="Sample Quantiles")

# Lack of Fit test

# Create Normal ANOVA 
fev.bc.aov.n <- aov(fev ~ age, data=fev.bc)
fev_aov_norm_summary <- summary(fev.bc.aov.n)

# Create Factored ANOVA
fev.bc.aov.f <- aov(fev ~ as.factor(age), data=fev.bc)
fev_aov_fac_summary <- summary(fev.bc.aov.f)

# State and Calculate Values
sse <- fev_aov_norm_summary[[1]]$`Sum Sq`[2]
sspe <- fev_aov_fac_summary[[1]]$`Sum Sq`[2]
sslf <- sse - sspe
c <- fev_aov_fac_summary[[1]]$Df[1]
n <- length(fev$fev)
mslf <- sslf/(c-2)
mspe <- sspe/(n-c)
F_lof <- mslf/mspe
pval <- pf(F_lof, (c-2), (n-c))

# Display Results
lof.results <- kable(data.frame("Source"=c("Residual Error", "Pure Error", "Lack of Fit"),
                 "Df"=c(n-2, c, ""),
                 "SS"=c(sse, sspe, sslf),
                 "MS"=c(sse/(n-2), mspe, mslf),
                 "F_Stat"=c(round(F_lof, 5), "", ""),
                 "P_Value"=c(round(pval, 5), "", "")), caption = "Lack of Fit Test")

ggarrange(fev.boxcox)
ggarrange(fev.scatter)
ggarrange(fev.resid, fev.residQQ)
lof.results
```

After performing a boxcox transformation of the FEV data using $\lambda=0.2$, the residuals now appear to be normally distributed and have equal variance. 

**9. Using the data set hw2chapter5p2.csv carry out a regression analysis with model fit assessment. If necessary, determine a transformation of either outcome $Y$ or predictor $X$ variable. Justify your choice with model assumptions and residual analysis or any other tests as appropriate.**
 
```{r}
data <- read.csv("../Data/hw2chapter5p2.csv")

# Make Linear Model
data.lm <- lm(outcome ~ predictor, data=data)
data.lm.summary <- summary(data.lm)

# Make scatterplot
data.scatter <- data%>%
  ggplot(aes(x=predictor, y=outcome))+
  geom_point(color='burlywood3')+
  geom_smooth(method='lm', color="black")+
  labs(title="Scatterplot",
       x="Predictor Variable",
       y="Response variable",
       subtitle=paste0("R-squared: ", data.lm.summary$r.squared))
```

From the linear model fitted between our response and predictor variables, there seems to be a linear relationship between the two variables and a strong positive correlation between the two variables. 

```{r}
# Create Residual Plot
data.resid <- data.lm%>%
  ggplot(aes(x=fitted(data.lm), y=residuals(data.lm)))+
  geom_point(color="darkgoldenrod", shape=21)+
  geom_hline(yintercept=0)+
  labs(title="Residuals Against Fitted Values",
       x="Fitted Values",
       y="Residuals")

# Create Residual QQ plot
data.residQQ <- data.lm%>%
  ggplot(aes(sample=residuals(data.lm)))+
  geom_qq(color="brown3", size=1.3)+
  geom_qq_line()+
  labs(title="Residual QQ Plot",
       x="Theoretical Quantiles",
       y="Sample Quantiles")

# Create Predictor QQ plot
data.predQQ <- data.lm%>%
  ggplot(aes(sample=data$predictor))+
  geom_qq(color="lightblue3", size=1.3)+
  geom_qq_line()+
  labs(title="Predictor QQ Plot",
       x="Theoretical Quantiles",
       y="Sample Quantiles")

# Create Response QQ plot
data.respQQ <- data.lm%>%
  ggplot(aes(sample=data$outcome))+
  geom_qq(color="forestgreen", size=1.3)+
  geom_qq_line()+
  labs(title="Response QQ Plot",
       x="Theoretical Quantiles",
       y="Sample Quantiles")

ggarrange(data.resid, data.residQQ)
ggarrange(data.respQQ, data.predQQ)
```

From the residual plot and the predictor QQ plot, we can conclude that both our data and residuals violate the normality assumption and require a boxcox transformation to achieve a more accurate model. 

## Chapter 6

**1. Consider the following regression problem: $E[Y] = 2 + 3X$ with $\epsilon_i \sim N(0, 4)$ and $\epsilon_i, \epsilon_j$ independent for $i, j = 1, 2, 3, 4$. Write the joint distribution of $\epsilon_1, \epsilon_2, \epsilon_3, \epsilon_4$. Note, the determinant of $\sum_\epsilon$ is given by $|\sum_\epsilon| = \sigma^2_{11} \cdot \sigma^2_{22} \cdot \sigma^2_{33} \cdot \sigma^2_{44}$.**

\begin{gather*}
f(\epsilon_1, ..., \epsilon_n) = \frac{1}{(2\pi)^{n/2}|\sum|^{1/2}}exp(-\frac{1}{2}(\epsilon_1, ..., \epsilon_n)\sum^{\mathop{}_{\mkern+55mu -1}}(\epsilon_1, ..., \epsilon_n)')
\\
= \frac{1}{(2\pi)^{2}(4^{2})}exp(-\frac{1}{2}\sum_{i=1}^{n}{\epsilon_i^2/\sigma_i^2})
\\
= \frac{1}{(8\pi)^{2}}exp(-\frac{1}{2}\sum_{i=1}^{4}{\epsilon_i^2/\sigma_i^2})
\end{gather*}

**2. Consider the following regression problem: $E[Y] = 2 + 3X$ with $\epsilon_i \sim N(0, 4)$ and $\epsilon_i, \epsilon_j$ independent for $i, j = 1, 2, 3, 4$. Write the joint distribution of $Y_1, Y_2, Y_3, Y_4$ for $X = 2, 4, 6, 8$. Based on your results in problem 1 you only need to calculate the expression in the exponent of the distribution for $Y_1, Y_2, Y_3, Y_4$.**

\begin{gather*}
Y =[8,14,20,26] 
\\
f(Y) = \frac{1}{(2\pi)^{n/2}|\sum|^{1/2}}exp(-\frac{1}{2}(Y-\mu)'\sum^{\mathop{}_{\mkern+55mu -1}}(Y-\mu))
\\
= \frac{1}{(8\pi)^{2}}exp(-\frac{1}{2}(
\begin{pmatrix}
-9 & 5 & 11 & 17
\end{pmatrix}
\sum^{\mathop{}_{\mkern+55mu -1}}
\begin{pmatrix}
-9 \\ 5 \\ 11 \\ 17
\end{pmatrix}
))
\end{gather*}

**3. Suppose, we have the following data: $(X_i, Y_i) = (88, 20); (70, 26); (72, 28); (64, 23)$ where $X$ is the percent of households (in a country) with high speed internet access and $Y$ is the amount of time in hours/week online. For this data write the linear regression model in matrix notation and provide the vector $Y$ as well as the matrix $X$.**

\begin{gather*}
Y = b_0 + b_1X + \epsilon_i
\\
\begin{pmatrix}
20\\
26\\
28\\
23
\end{pmatrix}
=
\begin{pmatrix}
1 & 88\\
1 & 70\\
1 & 72\\
1 & 64
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\end{pmatrix}
+
\epsilon_i
\end{gather*}

**4. For the data in problem 3 calculate the following quantities: $(X'X)$. Write the normal equations in matrix notation with explicit expressions for the quantities involved.**

\begin{gather*}
X'X = 
\begin{pmatrix}
1 & 1 & 1 & 1 \\
88 & 70 & 72 & 64
\end{pmatrix}
\begin{pmatrix}
1 & 88 \\ 
1 & 70 \\ 
1 & 72 \\ 
1 & 64
\end{pmatrix}
=
\begin{pmatrix}
4 & 294 \\
294 & 21924
\end{pmatrix}
\\
X'Xb = X'Y
\\
\begin{pmatrix}
4 & 294 \\
294 & 21924
\end{pmatrix}
\begin{pmatrix}
b_1 \\
b_2
\end{pmatrix}
=
\begin{pmatrix}
1 & 1 & 1 & 1 \\
88 & 70 & 72 & 64
\end{pmatrix}
\begin{pmatrix}
1 & 88 \\ 
1 & 70 \\ 
1 & 72 \\ 
1 & 64
\end{pmatrix}
\end{gather*}

**5. For the data in problem 3 calculate $(X'X)^{-1}$. Furthermore, calculate the least squares estimates $b_0$ and $b_1$.**

\begin{gather*}
(X'X)^{-1} = 
\frac{1}{1260}
\begin{pmatrix}
21924 & -294 \\
-294 & 4
\end{pmatrix}
=
\begin{pmatrix}
17.4 & -\frac{7}{30} \\
-\frac{7}{30} & \frac{1}{315}
\end{pmatrix}
\\
\begin{pmatrix}
b_0 \\ b_1
\end{pmatrix}
=
(X'X)^{-1}X'Y
= 
\begin{pmatrix}
17.4 & -\frac{7}{30} \\
-\frac{7}{30} & \frac{1}{315}
\end{pmatrix}
\begin{pmatrix}
1 & 1 & 1 & 1 \\
88 & 70 & 72 & 64
\end{pmatrix}
\begin{pmatrix}
20 \\ 26 \\ 28 \\ 23
\end{pmatrix}
=
\begin{pmatrix}
38.6 \\
-0.1952
\end{pmatrix}
\end{gather*}
 
**6. For the data in problem 3, calculate the hat matrix and the predicted values and residuals using the hat matrix**

\begin{gather*}
H = X(X'X)^{-1}X' =
\begin{pmatrix}
1 & 88 \\ 
1 & 70 \\ 
1 & 72 \\ 
1 & 64
\end{pmatrix}
\begin{pmatrix}
21924 & -294 \\
-294 & 4
\end{pmatrix}
\begin{pmatrix}
1 & 1 & 1 & 1 \\
88 & 70 & 72 & 64
\end{pmatrix}
= 
\begin{pmatrix}
0.9175 & 0.08889 & 0.1810 & -0.1873 \\
0.08889 & 0.2889 & 0.2667 & 0.3556 \\
0.1810 & 0.2667 & 0.2571 & 0.2952 \\
-0.1873 & 0.3556 & 0.2952 & 0.5365
\end{pmatrix}
\\
\widehat{Y} = HY
=
\begin{pmatrix}
0.9175 & 0.08889 & 0.1810 & -0.1873 \\
0.08889 & 0.2889 & 0.2667 & 0.3556 \\
0.1810 & 0.2667 & 0.2571 & 0.2952 \\
-0.1873 & 0.3556 & 0.2952 & 0.5365
\end{pmatrix}
\begin{pmatrix}
20 \\ 26 \\ 28 \\ 23
\end{pmatrix}
=
\begin{pmatrix}
21.4191 \\
24.9333 \\
24.5429 \\
26.1048
\end{pmatrix}
\\
e = Y - \widehat{Y} = Y - HY
=
\begin{pmatrix}
20 \\ 26 \\ 28 \\ 23
\end{pmatrix}
-
\begin{pmatrix}
21.4191 \\
24.9333 \\
24.5429 \\
26.1048
\end{pmatrix}
=
\begin{pmatrix}
-1.4190 \\
1.06667 \\
3.4571 \\
-3.1048
\end{pmatrix}
\end{gather*}

**7. Consider the data $(X_{1i}, X_{2i}, Y_i)$. Write down the design matrix for the following models**

\begin{gather*}
X_{1} =
\begin{pmatrix}
5.9\\
3.5\\
2.5\\
5.2\\
4.8\\
3.9\\
5.5\\
6.3\\
6.6
\end{pmatrix}
X_{2} = 
\begin{pmatrix}
3.0\\
1.9\\
2.9\\
2.8\\
4.6\\
3.3\\
1.7\\
3.3\\
2.7
\end{pmatrix}
Y=
\begin{pmatrix}
6.1\\
5.1\\
4.6\\
5.4\\
6.4\\
4.5\\
4.8\\
5.8\\
5.4
\end{pmatrix}
\end{gather*}

**a) $Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \epsilon_i$**

\begin{gather*}
Y_i = 
\begin{pmatrix}
1 & 3.0 & 6.1 \\
1 & 1.9 & 5.1 \\
1 & 2.9 & 4.6 \\
1 & 2.8 & 5.4 \\ 
1 & 4.6 & 6.4 \\
1 & 3.3 & 4.5 \\
1 & 1.7 & 4.8 \\
1 & 3.3 & 5.8 \\
1 & 2.7 & 5.4
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\ \beta_1 \\ \beta_2
\end{pmatrix}
+
\epsilon_i
\end{gather*}

**b) $Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \beta_{12}X_{1i}X_{2i} + \beta_3X^2_{1i} + \beta_4X^2_{2i} + \epsilon_i$**

\begin{gather*}
Y_i =
\begin{pmatrix}
1 & X_{11} & X_{21} & X_{11}X_{21} & X_{11}^2 & X_{21}^2\\
1 & X_{12} & X_{22} & X_{12}X_{22} & X_{12}^2 & X_{22}^2\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
1 & X_{19} & X_{29} & X_{19}X_{29} & X_{19}^2 & X_{29}^2
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_{12} \\ \beta_3 \\ \beta_4
\end{pmatrix}
+
\epsilon_i
\\
Y_i = 
\begin{pmatrix}
1 & 3.0 & 6.1 & 17.70 & 34.81 & 9.00\\
1 & 1.9 & 5.1 & 6.65 & 12.25 & 3.61\\
1 & 2.9 & 4.6 & 7.25 & 6.25 & 8.41\\
1 & 2.8 & 5.4 & 14.56 & 27.04 & 7.84\\ 
1 & 4.6 & 6.4 & 22.08 & 23.04 & 21.16\\
1 & 3.3 & 4.5 & 12.87 & 15.21 & 10.89\\
1 & 1.7 & 4.8 & 9.35 & 30.25 & 2.89\\
1 & 3.3 & 5.8 & 20.79 & 39.69 & 10.89\\
1 & 2.7 & 5.4 & 17.82 & 43.56 & 7.29
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_{12} \\ \beta_3 \\ \beta_4
\end{pmatrix}
+
\epsilon_i
\end{gather*}
